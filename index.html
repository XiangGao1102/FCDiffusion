<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>FCDiffusion-AAAI2024</title>
    <style type="text/css">
        body{
        	background-color: white;
        }
        .links{
        	text-decoration: none;
        	color: #0066CC;
        }
        .p2{
        	padding-top: 20px;
        	font-size: 25px;
        }
        .p1{
        	text-align:justify;
        	text-justify:inter-ideograph;
        }
		
		.left {
			text-align: left;
			border: 1px dotted black;
			width: 50%;
		}
        a{
        	font-family: Sans-serif;
        }
        p{
        	font-family: Sans-serif;
        }
        ul{
        	font-family: Sans-serif;
        }
    </style>
</head>
<body>
	<div align="center" style="padding-top: 30px;">
	<p style="font-size:35px;">Frequency-Controlled Diffusion Model for Versatile <br>
		Text-Guided Image-to-Image Translation </p>

	<a href="mailto:gaoxiang1102@pku.edu.cn" class="links">Xiang Gao </a> &nbsp; &nbsp; 
	<a href="mailto:icey.x@pku.edu.cn" class="links">Zhengbo Xu </a> &nbsp; &nbsp; 
	<a href="mailto:liujiaying@pku.edu.cn" class="links">Jiaying Liu</a>

	<br>
	<p class="para-3"><span class="p1"> Wangxuan Institute of Computer Technology, Peking University, Beijing, China</span></p>
	<p class="para-3"><span class="p1"> Accepted by <i>AAAI 2024.</i></span></p>
	

	</div>

        <div align="left" style="padding-left: 15%; padding-right: 15%; padding-bottom: 30px;">
		<p class='p2'> Abstract </p> 
		<p class='p1'>Contrastive learning has been proven beneficial for self-supervised skeleton-based action recognition. Most contrastive learning methods utilize carefully designed augmentations to generate different movement patterns of skeletons for the same semantics. However, it is still a pending issue to apply strong augmentations, which distort the images/skeletons' structures and cause semantic loss, due to their resulting unstable training. In this paper, we investigate the potential of adopting strong augmentations and propose a general hierarchical consistent contrastive learning framework (HiCLR) for skeleton-based action recognition. Specifically, we first design a gradual growing augmentation policy to generate multiple ordered positive pairs, which guide to achieve the consistency of the learned representation from different views. Then, an asymmetric loss is proposed to enforce the hierarchical consistency via a directional clustering operation in the feature space, pulling the representations from strongly augmented views closer to those from weakly augmented views for better generalizability. Meanwhile, we propose and evaluate three kinds of strong augmentations for 3D skeletons to demonstrate the effectiveness of our method. Extensive experiments show that HiCLR outperforms the state-of-the-art methods notably on three large-scale datasets, i.e., NTU60, NTU120, and PKUMMD.</p>
            </div>
			
        <div align="left" style="padding-left: 15%; padding-right: 15%; padding-bottom: 30px;">
            <p class='p2'> Method </p> 
			<p style="line-height:180%">
			<strong>Key Idea</strong>: 
			<br>
			(1) <i>Growing Augmentation</i>. All data augmentations are divided into multiple sets. We apply these sets progressively to the skeleton data to generate the augmented samples corresponding to different augmented strengths.
			<br>
			(2) <i> Hierarchical Consistent Learning</i>. Then, the model modelling the consistency of these positive pairs by constraining the similarity of the samples generated by the augmentations with adjacent strengths.
			</p>
			
			<br>
            <div style="padding-left: 5%; padding-right: 5%;">
                <div align="center">
                    <img src="img/arch.jpg" width="100%"> <br>
                </div>
            <p style="line-height:180%">Figure 1. The overview architecture of the proposed HiCLR. There are <i>k</i> branches corresponding to the different augmentation invariance learning. We employ a growing augmentation strategy to generate multiple highly correlated positive pairs corresponding to different augmented strengths.  The augmented views are encoded via the query/key encoder and projector. Meanwhile, a hierarchical self-supervised loss is proposed to align the feature distributions of adjacent branches, which is optimized jointly with the InfoNCE loss.
            
			</div>
        
            <p class='p2'> Results </p> 
            <div style="padding-left: 5%; padding-right: 5%;">
                <div align="center">
					<p style="line-height:150%">
					Table 1. Linear evaluation results on NTU60 and NTU120 datasets.
					</p>
                    <img src="img/result.jpg" width="100%"> <br>
                </div>
			<br>

                <div align="center">
					<p style="line-height:150%">
					Table 2. Supervised evaluation results on NTU60 and NTU120 datasets.
					</p>
                    <img src="img/sup_result.jpg" width="50%"> <br>
                </div>
			</div>
		

	<p class='p2'> Resources </p> 
	<p class='p1'>
		<ul style="line-height:15px">
		　　<li> Paper: <a href="https://arxiv.org/abs/2211.13466" class="links">arXiv</a> </li>
		　　<!--<li> Supplementary: <a href="https://drive.google.com/open?id=1QDMWbhw2jgutDsLaS00R-P6cxZ_OaZis" class="links">Google Drive</a>, <a href="https://pan.baidu.com/s/1ke9hqo62pVhl6_6YqAA4cQ" class="links">Baidu Pan</a> (Code: wvr6) </li>-->

		　　<li> <a href="https://github.com/JHang2020/HiCLR" class="links">Code</a> </li>
		</ul>
	</p>

	<p class='p2'> Citation</p>
	<p> 
		@article{zhang2023hierarchical, <br>
			&nbsp; &nbsp; title={Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations},<br>
			&nbsp; &nbsp; author={Zhang, Jiahang and Lin, Lilang and Liu, Jiaying},<br>
			&nbsp; &nbsp; booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
			&nbsp; &nbsp; year={2023}, <br>
			}
	</p>

	<p class='p2'> Reference</p>
	<p> 
		[1] Liu, J.; Song, S.; Liu, C.; Li, Y.; and Hu, Y. A benchmark dataset and comparison study for multi-modal human action analytics. <i>TOMM</i> 2020. <br> <br>
		[2] Shahroudy, A.; Liu, J.; Ng, T.-T.; and Wang, G. Ntu rgb+ d: A large scale dataset for 3d human activity analysis. <i>CVPR</i> 2016. <br> <br>
		[3] Liu, J.; Shahroudy, A.; Perez, M.; Wang, G.; Duan, L.-Y.; and Kot, A. C. NTU RGB + D 120: A large-scale benchmark for 3D human activity understanding. <i>TPAMI</i> 2019. <br> <br>
		[4] Guo, T.; Liu, H.; Chen, Z.; Liu, M.; Wang, T.; and Ding, R. Contrastive Learning from Extremely Augmented Skeleton Sequences for Self-supervised Action Recognition. <i>AAAI</i> 2022. <br> <br>
		[5] Lin, L.; Song, S.; Yang, W.; and Liu, J. MS2L: Multi-task self-supervised learning for skeleton based action recognition. <i>ACM MM</i> 2020.
	</p>
	
	<p class='left'>
		<ul style="line-height:15px">
			Return to the <a href="http://39.96.165.147/Projects.html" class="links">STRUCT Project</a>
		</ul>
	</p>
		

</html>
